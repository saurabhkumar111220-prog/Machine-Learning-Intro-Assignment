{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning Intro | Assignment"
      ],
      "metadata": {
        "id": "qN86ZIIgjAu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).\n",
        "=> Artificial Intelligence (AI) is the broad field of making machines mimic human intelligence. Machine Learning (ML) is a subset of AI where systems\n",
        "    learn patterns from data. Deep Learning (DL) is a specialized branch of ML using neural networks. Data Science (DS) is the broader discipline of\n",
        "    extracting insights from data, often using AI/ML as tools"
      ],
      "metadata": {
        "id": "wFDqKPUAjSoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.What are the types of machine learning? Describe each with one real-world example.\n",
        "=> The main types of machine learning are #.Supervised Learning, #.Unsupervised Learning, #.Reinforcement Learning, and #.Semi-Supervised Learning.\n",
        "   Each type differs in how the model learns from data and applies knowledge to real-world problems.\n",
        "   Type                        Description                                   Real-World Example\n",
        "\n",
        "   Supervised Learning          Model learns from labeled data               Email spam filters trained on\n",
        "                                (input-output pairs) to predict outcomes     “spam” vs “not spam” labels\n",
        "\n",
        "   Unsupervised Learning         Model finds hidden patterns in unlabeled     Customer segmentation in marketing (grouping buyers by behavior)\n",
        "                                 data.\n",
        "\n",
        "   Reinforcement Learning       Agent learns by interacting with environment,  Self-driving cars learning to navigate safely\n",
        "                                 receiving rewards or penalties\n",
        "\n",
        "   Semi-Supervised Learning      Model trained on small labeled data + large    Medical imaging where only a few scans are labeled\n",
        "                                 unlabeled data .\n",
        "\n",
        "   Self-Supervised Learning      Model generates its own labels from raw data,  Large language models predicting missing words in text\n",
        "                                  often used in modern AI"
      ],
      "metadata": {
        "id": "1QbvaiBCkNes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Define overfitting, underfitting, and the bias-variance tradeoff in machine learning.\n",
        "=> Overfitting= When a model learns the training data too well, including noise and random fluctuations, making it perform poorly on unseen data.\n",
        "\n",
        "=> Underfitting= When a model is too simple to capture the underlying patterns in the data, leading to poor performance both on training and test sets.\n",
        "\n",
        "=> Bias-Variance Tradeoff= The balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in training data).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4ZrqbOXowq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.What are outliers in a dataset, and list three common techniques for handling them.\n",
        "=> Outliers are data points that differ significantly from the majority of observations in a dataset.\n",
        "   Three Common Techniques for Handling Outliers\n",
        "   Removal,Transformation,Imputation or Capping\n",
        "\n"
      ],
      "metadata": {
        "id": "MqKKAdXhpmYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.\n",
        "=> Great question! Missing values are a common challenge in data preprocessing, and handling them properly is crucial for building reliable machine\n",
        "   learning models. Let’s break it down.\n",
        "   Imputation Techniques;\n",
        "   Numerical Data → Mean Imputation\n",
        "   Categorical Data → Mode Imputation\n",
        "\n"
      ],
      "metadata": {
        "id": "-UjzRoxjqWVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Creates a synthetic imbalanced dataset with make_classification() from sklearn.datasets.\n",
        "=> I generated a synthetic imbalanced dataset with 1000 samples and 20 features, using make_classification() from sklearn.datasets.\n",
        "   The class distribution is 898 samples of class 0 and 102 samples of class 1, reflecting the imbalance ratio of 90:10.\n",
        "\n",
        "   Samples: 1000\n",
        "\n",
        "Features: 20 (2 informative, 10 redundant, 8 noise)\n",
        "\n",
        "Classes: 2 (binary classification)\n",
        "\n",
        "Weights: [0.9, 0.1] → 90% majority class, 10% minority class\n",
        "\n",
        "Random State: 42 (for reproducibility)\n"
      ],
      "metadata": {
        "id": "AK7b_BdirFIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Implement one-hot encoding using pandas for the following list of colors: ['Red', 'Green', 'Blue', 'Green', 'Red']. Print the resulting dataframe.\n",
        "=> import pandas as pd\n",
        "\n",
        "# Original list of colors\n",
        "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
        "\n",
        "# Create a DataFrame\n",
        "color_df = pd.DataFrame(colors, columns=['Color'])\n",
        "\n",
        "# Apply one-hot encoding\n",
        "encoded_df = pd.get_dummies(color_df, columns=['Color'])\n",
        "\n",
        "print(encoded_df)\n",
        "\n",
        "Resulting DataFrame\n",
        "\n",
        "Color_Blue\tColor_Green\tColor_Red\n",
        "0\t0\t0\t1\n",
        "1\t0\t1\t0\n",
        "2\t1\t0\t0\n",
        "3\t0\t1\t0\n",
        "4\t0\t0\t1\n"
      ],
      "metadata": {
        "id": "w89Q-jvLsEy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python script to:\n",
        "● Generate 1000 samples from a normal distribution.\n",
        "● Introduce 50 random missing values.\n",
        "● Fill missing values with the column mean.\n",
        "● Plot a histogram before and after imputation.\n",
        "\n",
        "=> Step 1: Generated 1000 samples from a normal distribution.\n",
        "\n",
        "Step 2: Introduced 50 random missing values.\n",
        "\n",
        "Step 3: Filled missing values with the column mean.\n",
        "\n",
        "Step 4: Plotted histograms before and after imputation for comparison.\n",
        "\n",
        "The visualization below shows the distribution before imputation (left) and after imputation (right):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2kMzP18VsxyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Implement Min-Max scaling on the following list of numbers [2, 5, 10, 15,\n",
        "20] using sklearn.preprocessing.MinMaxScaler. Print the scaled array.\n",
        "=> import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Original data\n",
        "numbers = np.array([2, 5, 10, 15, 20]).reshape(-1, 1)\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform data\n",
        "scaled_numbers = scaler.fit_transform(numbers)\n",
        "\n",
        "print(scaled_numbers.flatten())\n",
        "output:\n",
        "[0.         0.15789474 0.42105263 0.68421053 1.        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "TjDWMGCLt0Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.You are working as a data scientist for a retail company. You receive a customer\n",
        "transaction dataset that contains:\n",
        "● Missing ages,\n",
        "● Outliers in transaction amount,\n",
        "● A highly imbalanced target (fraud vs. non-fraud),\n",
        "● Categorical variables like payment method.\n",
        "Explain the step-by-step data preparation plan you’d follow before training a machine learning\n",
        "model. Include how you’d address missing data, outliers, imbalance, and encoding.\n",
        "=> Before modeling, I’d create a reproducible pipeline that prevents leakage and keeps evaluation honest. The plan below follows a logical\n",
        "   sequence you can implement with train/test splits, cross-validation, and feature transformers applied only on training data.\n",
        "\n",
        "   Data audit and split\n",
        "\n",
        "   #Profile data:\n",
        "\n",
        "Explore missingness patterns (MCAR/MAR/MNAR), outlier distributions, class imbalance ratio, and categorical cardinality.\n",
        "\n",
        "Check duplicates, inconsistent IDs, and timestamp logic (e.g., transaction after account creation).\n",
        "\n",
        "   Define target and leak checks:\n",
        "\n",
        "Remove or delay-post features that reflect post-transaction outcomes (e.g., chargebacks) to avoid leakage.\n",
        "\n",
        "Stratified split:\n",
        "\n",
        "Split into train/validation/test using stratification on fraud vs. non-fraud to preserve class ratios.\n",
        "\n",
        "All preprocessing steps (imputation, scaling, encoding, resampling) are fit on training only and applied to validation/test.\n",
        "\n",
        "\n",
        "#Handling missing ages:\n",
        "\n",
        " Assess mechanism:\n",
        "\n",
        "Examine missingness by customer segment, payment method, device type; if age is MNAR (e.g., certain cohorts skip age), avoid naive imputation that biases distributions.\n",
        "\n",
        "Imputation strategy:\n",
        "\n",
        "Numerical imputation:\n",
        "\n",
        "Primary: Median or robust imputation within meaningful groups (e.g., by customer segment or region) to preserve distribution.\n",
        "\n",
        "Model-based: Train a simple regressor (e.g., LightGBM or linear model) to predict age from stable features if missingness is substantial.\n",
        "\n",
        "Indicator flag:\n",
        "\n",
        "Add a binary “age_missing” feature to capture the signal of missingness itself.\n",
        "\n",
        "Validation:\n",
        "\n",
        "Compare distributions before/after; ensure no unrealistic ages are introduced.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AXmBUuzkvGHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}